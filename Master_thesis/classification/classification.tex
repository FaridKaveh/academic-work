\section{Classification of Non-Markov Processes}\label{chapter:classification}
\textit{\textbf{Notational Warning:} In this section $\Omega$ no longer refers to the path space of a stochastic process. Instead, $(\Omega, \mathcal{F}, \bP)$ refers to a generic probability space}

\textit{In this section we conclude the text with a reflection on the nature of the stochastic processes considered. We propose a novel classification of non-Markov processes to guide future research in the area. It is hoped also that the following discussion can help to bridge the gap between the mathematical discipline of Stochastic Processes and the study of such processes in physics.}

By way of reflection and to motivate future study, It is pertinent to present a classification of the types of non-Markov processes considered here. Such a classification is helpful because the class of processes that can be considered non-Markov is very broad. Throughout Sections \ref{chapter:telegraph}-\ref{chapter:RnT} we have developed techniques for analysing processes that fall within the different categories described in this section. In this way, the following categorisation can also guide future work on non-Markov stochastic processes by helping to identifying useful techniques in each case. In the sequel, we shall assume $\mathcal{X}$ to be a \emph{Polish} space (meaning that $\mathcal{X}$ allows a seperable and complete metrisation) and that it is endowed with its Borel $\sigma$-algebra. First, let us recall that a Markov process $X_t: \Omega \times [0,+\infty) \rightarrow \mathcal{X}$, is one that satisfies 

\begin{align}\label{Markov-property}
\bE[f(X_t) \lvert \mathcal{F}_s] = \bE[f(X_t) \lvert X_s], \quad s \leq t.
\end{align}

Here $\mathcal{F}_t = \sigma \left(\{X_s: \, s \leq t \}\right)$ is the $\sigma$-algebra generated by $X_t$ up to time $t$ and $f: \mathcal{X} \rightarrow \bR$ is any bounded measurable function. The classifications that follow are not meant for dividing non-Markov processes into mutually exclusive groups. Indeed, if a process fulfils one classification that does not preclude it from satisfying also the others. Instead, these groupings identify unifying paradigms that will be helpful in the study of non-Markov processes in future.

\subsection{Quasi-Markov Processes}
To motivate this first category, consider the following example from \cite{XuMeiMarkov}. Let $\{Y_n\}_{n=0}^\infty$ be i.i.d random variables taking value in $\{1,2,\ldots, 20\}$. Define $x_0 = Y_0$, $x_1 = Y_1$, and $x_{n+1} = x_n + x_{n-1} + Y_{n+1}$ for $n \geq 1$. Also define $z_n = (x_n, x_{n-1})$. It is easily checked that $x_n$ is not Markov -for example $x_2$ depends on $x_0$ and $x_1$ which are themselves independent- while $z_n$ is Markov. In this example, the non-Markov nature of $x_n$ is rather trivial. It is a matter of description. The process can be made Markov by simply keeping a record of its history up to a constant time before the present. This is achieved with the r.v. $z_n$. 

More generally, let $T \in [0, \infty)$ be a constant time. Define the future of $X_t$, $\mathcal{F}_t^\ast = \sigma\left(\{X_s: s \geq t\}\right)$. Let $X_t$ be a process satisfying 

\begin{align}\label{quasi-Markov}
\bE[f(X_t) \lvert \mathcal{F}_s] = \bE[f(X_t) \lvert \mathcal{F}^\ast_{s-T}\cap\mathcal{F}_s], \quad T \leq s.
\end{align}

We shall refer to processes satisfying Eqn. (\ref{quasi-Markov}) for some $T$ as quasi-Markov processes. The process $x_n$ defined above is a discrete-time quasi-Markov process. Other examples of quasi-Markov processes include time-delayed DEs and Stochastic Differential Delay Equations (SDEEs). For a quasi-Markov process $X_t$, we define the \textit{Markov description} of $X_t$,

\begin{align}\label{Markov-description}
Z_t \coloneqq \left \{X_s\right\}_{s=t}^{t+T} : \Omega \times [0, +\infty) \rightarrow \mathcal{X}^{[0,T]}.
\end{align}

As the name suggests, $Z_t$ is a Markov process. In general it is an infinite dimensional process, but for discrete-time processes it is $T$-dimensional. In the first motivating example for this section, $z_n$ is the Markov description of $x_n$. What we call the Markov description of $X_t$ has been implicitly used to study systems with delayed dynamics using the framework of functional differential equations\cite{kolmanovskii2013introduction,richard2003time}.  When $T = 0$, then $\mathcal{F}^\ast_{s-T}\cap\mathcal{F}_s = \sigma(\{X_s\})$, hence Eqn. (\ref{quasi-Markov}) becomes 

\begin{align}
\bE[f(X_t) \lvert \mathcal{F}_s] = \bE[f(X_t) \lvert X_s], 
\end{align}

so $X_t$ is Markov. We have not not considered any quasi-Markov systems in the preceding sections, nevertheless they comprise an important category of non-Markov processes. 

\subsection{Semi-Markov Processes}
By way of comparison with a quasi-Markov process, consider the `Waiting Room' (WR) process described in Section \ref{chapter:waiting-room}. Refer to Section \ref{chapter:waiting-room} for a full description of this process. Here we let $R_t$ denote the WR process. The state-space for this process is $\mathcal{X} = \{\ket{1},\ket{3}, \ket{w(t)} = a(t)\ket{2} + b(t)\ket{4} \}$ where $0<a(t),b(t)<1$ and $a(t)+b(t) =1$. Whenever the system occupies states $\ket{1}$ or $\ket{3}$, its evolution is independent of the past. However, when the system occupies $\ket{w}$ its time evolution depends on the past up to the last instance when either of the states $\ket{1}$ or $\ket{3}$ were observed (see Eqn. (\ref{waiting-room-timeevol})). In this case, if we were to use all available information to predict the future of $R_t$, our record keeping would need to extend until the first such instance. But how long ago the process occupied either of $\ket{1}$ or $\ket{3}$ is itself a stochastic variable. This is unlike the first example with the process $x_n$. In the case of $x_n$ the time-evolution of the process is affected by a \textit{constant} time-delay. The time-delay in the case of $R_t$ varies stochastically. 

We now let $T$ be a stopping time with respect to the filtration $\{\mathcal{F}_t\}_{t=0}^\infty$. A stopping time\newline $T: \Omega \rightarrow [0, +\infty]$ with respect to $\{\mathcal{F}_t\}_{t=0}^\infty$ is a random variable such that the event $\{T \leq t\}$ is $\mathcal{F}_t$ measurable for all $t$. The stopped $\sigma$-algebra of $X_t$ with respect to $T$ is the set 

\begin{align}
\mathcal{F}_T = \left\{A \in \mathcal{F}_\infty : A \cap \{T \leq t\} \in \mathcal{F}_t, \quad \forall t > 0\right\}\end{align}

Intuitively, $\mathcal{F}_T$ is comprised of all the information about the process $X_t$ up to time $T$. Suppose that $T$ is almost surely finite, $\bP( T < \infty) = 1$. Then we will say that $X_t$ is a semi-Markov process if

\begin{align}\label{semi-Markov-property}
\bE[f(X_{T+t})\lvert\mathcal{F}_T] =  \bE[f(X_{T+t}) \lvert X_T].
\end{align}

for some stopping time $T$. When Eqn. (\ref{semi-Markov-property}) holds for all stopping times $T$ with respect to $\{\mathcal{F}_t\}_{t=0}^\infty $, then $X_t$ is said to have the strong Markov property \cite{durrett2019probability} which is a stronger formulation of the Markov property (Eqn. (\ref{Markov-property})). When a semi-Markov process is restarted at time $T$, then its future, conditioned on the present, is independent of the past. As we shall see in Section \ref{chapter:waiting-room}, a useful technique when considering semi-Markov processes is to restrict to the stopped process $X_{\min(t,T)}$. Since we can think of the process as restarting after time $T$, many quantities of interest are captured adequately by $X_{\min(t,T)}$. 

The WR process is a semi-Markov process. Let 

\begin{align}
T_1 = \inf \{t \geq 0: R_t = \ket{1}\}.
\end{align}

Since the Poisson jump rates away from $\ket{1}$ are independent of how the process came to this state, the semi-Markov property 

\begin{align}
\bE[f(R_{T_1+t})\lvert\mathcal{F}_{T_1}] =  \bE[f(R_{T_1+t}) \lvert R_{T_1} = \ket{1}],
\end{align}

holds. Likewise $R_t$ also obeys the semi-Markov property with respect to the stopping time 

\begin{align}
T_3 = \inf\{ t \geq 0: R_t = \ket{3}\}.  
\end{align}

\subsection{Lego Brick Processes}
 In Section \ref{chapter:UL1} we derive the path probabilities of the `Unrequited Love' (UL) system. This is a two-state Poisson process where the jump rate depends on the state of another Poisson process. We begin with a continuous-time Markov chain taking values in   $\{\ket{11},\ket{12},\ket{21},\ket{22}\}$ and driven by the master equation
 
 \begin{align}
\dot{P}(t) = e^{tW}P(t), \; \; \;  W = \begin{pmatrix} -\alpha - \beta & \alpha & \beta + \gamma & 0 \\
  \alpha & -\alpha -\beta -\gamma & 0 & \beta \\
  \beta & 0 & -\alpha-\beta-\gamma & \alpha \\
  0 & \beta+\gamma & \alpha & -\alpha-\beta\end{pmatrix},
\end{align}

where $\alpha$, $\beta$, and $\gamma$ are positive constants. With respect to the phase space, $W$ has the basis ordering $e_1 = \ket{11}$, $e_2 = \ket{21}$, $e_3 = \ket{12}$, and $e_4 = \ket{22}$, so that, for example, $W_{12}$ is the rate for the transition $\ket{21} \rightarrow \ket{11}$. Denote this Markov-chain by $Z_t$. We then collapse the state $\ket{11}$ into $\ket{12}$ to obtain the coarse-grained state $\ket{1}$. We also collapse $\ket{21}$ with $\ket{22}$ to obtain $\ket{2}$. The resulting two-state process, denoted henceforth by $U_t$, is the UL system. See Section \ref{chapter:UL1} for a more thorough description. 

The process $U_t$ is non-Markov. Indeed, the time spent by $U_t$ in either of the states $\ket{1}$ or $\ket{2}$ informs the observer of the underlying state of $Z_t$. The transition probabilities for $U_t$ are then updated based on the inferred state of $Z_t$. $U_t$ can be viewed as the coupling of two Poisson process in the following sense. Let $N_t$ be a symmetric telegraph process with rate $\alpha$ taking values in $ \{0,\gamma\}$. Then for small time $\delta t$

\begin{align}
\begin{split}
\bP\left(U_{t+\delta t} = \ket{i} \: \lvert \: U_t = \ket{j}, N_t = n  \right) = (\beta + n)\delta t, \quad i \neq j.
\end{split}
\end{align}


This is equivalent to 


\begin{align}
\begin{split}
\bE\left(f\left(U_{t+\delta t}\right)\: \lvert \: U_t = \ket{j}, N_t = n  \right) = f\left(\ket{i}\right)(\beta + n)\delta t - f\left(\ket{j}\right)(1 - (\beta + n)\delta t) , \quad i \neq j.
\end{split}
\end{align}

In other words, the generator of the process $U_t$ is a function of $N_t$. It is in this sense that we speak of the coupling of processes. In Section \ref{chapter:UL1} we were interested in the case where $Y_t$ is hidden fom the observer, meaning the situation where all available information is contained in the filtration generated by $U_t$. In general, let $(X_t,Y_t): \Omega \times [0, +\infty) \rightarrow \mathcal{X}\times \mathcal{Y}$ be a random variable such that $(X_t,Y_t)$ is Markov and moreover, for any $g: \mathcal{Y} \rightarrow \bR$ measurable there holds

\begin{align}
\bE[g(Y_t) \: \lvert \: \sigma(\{X_u,Y_u: u \leq s\})] &= \bE[g(Y_t) \:\lvert \:Y_s], \\
\bE[f(X_t) \: \lvert \sigma(\{X_u,Y_u: u \leq s\})]  &= \bE[f(X_t) \:\lvert \: \sigma(X_s,Y_s) ],
\end{align}

for all $s \leq t$. Such a process we call a `Lego Brick' process (thinking of processes as Lego bricks, $X_t$ is built on top of $Y_t$). The non-trivial cases are those where $\bE[f(X_t) \:\lvert \: \sigma(X_s,Y_s) ] \neq \bE[f(X_t) \:\lvert \: X_s ]$. Identifying $(U_t, N_t)$ with $(X_t, Y_t)$ as in the above, the UL processes is a Lego brick process. Recall the asymmetric RnT process described in Section \ref{chapter:RnT},
\begin{align}
dx_t = w_t \rmd t + \sqrt{2D}dB_t
\end{align}

where $B_t$ is Brownian motion and $w(t,\omega)$ is a stochastic process with bounded measurable sample paths. Here $(x_t, w_t)$ is a Lego brick process. Considering again the generic process $(X_t, Y_t)$, let us call $X_t$ the `follower' (process) and $Y_t$ the `leader' (process). In general, the paths of the follower become non-Markov in the absence of information about the leader. The most intuitive approach in dealing with this complication is to integrate over the paths of the leader. Indeed, this has been our approach in Sections \ref{chapter:UL1} and \ref{chapter:RnT}. This technique makes implicit use of the following proposition from Probability Theory \cite{XueMeiAltman2020}.\footnote{See Appendix A, Proposition A.5 for a proof of this statement.}

\begin{prop} 
Let $X: \Omega \rightarrow \mathcal{X}_1$ be $\mathcal{F}^\prime \subset \mathcal{F}$ measurable and let $Y: \Omega \rightarrow \mathcal{Y}$ be independent of $\mathcal{F}^\prime$. Let $\psi: \mathcal{X}\times \mathcal{Y} \rightarrow \bR$ be $\mathcal{B}(\mathcal{X}) \times \mathcal{B}(\mathcal{Y})$ measurable such that $\bE \abs{\psi(X,Y)} < \infty$. Define $h_\psi(x) \coloneqq \bE[\psi(x,Y)] = \int_\Omega \psi(x,Y(\omega)) \rmd\bP(\omega)$. Then 

\begin{align}
\bE\left[\psi(X,Y) \: \lvert \: \mathcal{F}^\prime\right] = h_\psi(X).
\end{align}
\end{prop}

To apply this result to our generic Lego brick process we identify

\begin{align}
\begin{split}
X &\longleftrightarrow \bE\left[X_t \: \lvert \: X_s \right] \\
Y &\longleftrightarrow \bE\left[Y_t \: \lvert \: Y_s \right] \\ 
\mathcal{F}^\prime &\longleftrightarrow \sigma(X_s).
\end{split}
\end{align}